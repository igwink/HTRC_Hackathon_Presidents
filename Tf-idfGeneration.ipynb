{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code more efficiently generates tf-idf scores by using Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BMLtQz0kMYEI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Windows\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "# import json\n",
        "# import math\n",
        "import re\n",
        "# import nltk\n",
        "import bz2\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load and Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ltk-OFB5rXEw"
      },
      "outputs": [],
      "source": [
        "# Sample workset\n",
        "\n",
        "# workset = {\n",
        "#     \"osu.32435051242188\": \"Truman\",\n",
        "#     \"osu.32437000791067\": \"Hoover\",\n",
        "#     \"uc1.31210014062952\": \"Clinton\",\n",
        "#     \"osu.32435057459778\": \"Kennedy\",\n",
        "#     \"osu.32435030026934\": \"Nixon\",\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i_xou8TMpsbz"
      },
      "outputs": [],
      "source": [
        "def post_workset(payload):\n",
        "  ''' Post a workset to the HTRC Extracted Features API and return the workset ID '''\n",
        "  url = \"https://data.htrc.illinois.edu/ef-api/worksets\"\n",
        "\n",
        "  headers = {\n",
        "      \"Content-Type\": \"text/plain\",\n",
        "      \"Accept\": \"application/json\"\n",
        "  }\n",
        "\n",
        "  response = requests.post(url, data=payload, headers=headers)\n",
        "  response_json = response.json()\n",
        "\n",
        "  workset_id = response_json['data']['id']\n",
        "\n",
        "  return workset_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rUCtYcS4oTur"
      },
      "outputs": [],
      "source": [
        "def get_workset(workset_id, save_data=False, file_name='USA Presidential Papers'):\n",
        "  '''Uses the HTRC Extracted Features API to retrieve volume data for a workset ID.'''\n",
        "  api_get_volume = \"https://data.htrc.illinois.edu/ef-api/worksets/{}/volumes/aggregated\".format(workset_id)\n",
        "  \n",
        "  response = requests.get(api_get_volume)\n",
        "  workset_data = response.json()\n",
        "\n",
        "  if save_data and workset_data[\"code\"] == 200:\n",
        "    with bz2.BZ2File(file_name + '.pbz2', 'wb') as f:\n",
        "      pickle.dump(workset_data, f)\n",
        "    \n",
        "  if workset_data[\"code\"] == 200:\n",
        "    print(\"Successfully retrieved data from API\")\n",
        "    return workset_data\n",
        "  else:\n",
        "    print(f'Failed to retrieve data: {workset_data[\"code\"]}')\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uNgRGypNq2Uu"
      },
      "outputs": [],
      "source": [
        "def get_workset_volumes_aggregated(workset_data):\n",
        "  '''Extracts the volume IDs and feature data from the workset data'''\n",
        "  workset_total_wc = {} # {volume_id:{word:count}}\n",
        "  for volume in workset_data['data']:\n",
        "    workset_total_wc['{}'.format(volume_id)] = volume['features']['body']\n",
        "\n",
        "  return workset_total_wc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_workset_data(file_name='USA Presidential Papers.pbz2'):\n",
        "  '''Loads the workset data from a file'''\n",
        "  with bz2.BZ2File(file_name, 'rb') as f:\n",
        "    workset_data = pickle.load(f)\n",
        "  \n",
        "  return workset_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whrNOCffw2ui",
        "outputId": "18c4719c-5cb8-4d72-b905-2fbcbeb33c32"
      },
      "outputs": [],
      "source": [
        "# workset_id = post_workset(workset.keys())\n",
        "# workset_data = get_workset('664e9a5938000014012eeda9', save_data=False)\n",
        "\n",
        "workset_data = load_workset_data('USA Presidential Papers.pbz2')\n",
        "\n",
        "workset_wc = get_workset_volumes_aggregated(workset_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine word counts for total workset and clean data\n",
        "wc_dict = {}\n",
        "for volume in workset_wc.keys():\n",
        "  wc_dict[volume] = {}\n",
        "  for term in workset_wc[volume].keys():\n",
        "    lower_term = term.casefold() # Convert to lowercase for case-insensitive comparison. More aggressive than .lower()\n",
        "    match = re.search(r'[^a-z]', lower_term) # Matches any non-alphabetic character. This excludes words with hyphens, apostrophes, etc. such as \"zero-sum\"\n",
        "    if match is not None: # Skip if regex match\n",
        "      continue\n",
        "    if lower_term in wc_dict[volume].keys():\n",
        "      wc_dict[volume][lower_term] += workset_wc[volume][term]\n",
        "    else:\n",
        "      wc_dict[volume][lower_term] = workset_wc[volume][term]\n",
        "\n",
        "# Convert dict to list for adding to dataframe\n",
        "volume_id_list = []\n",
        "term_list = []\n",
        "count_list = []\n",
        "for volume in wc_dict.keys():\n",
        "  for term in wc_dict[volume].keys():\n",
        "    volume_id_list.append(volume)\n",
        "    term_list.append(term)\n",
        "    count_list.append(wc_dict[volume][term])\n",
        "corpus_df = pd.DataFrame({'term': term_list, 'volume_id': volume_id_list, 'count': count_list})\n",
        "\n",
        "# Drop terms with less than 5 occurrences. These are likely OCR errors.\n",
        "corpus_df = corpus_df[corpus_df['count'] >= 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "nzThzglA0IXm",
        "outputId": "5dd6e723-7626-4720-e246-750e9520401b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>term</th>\n",
              "      <th>volume_id</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8864296</th>\n",
              "      <td>zz</td>\n",
              "      <td>uiug.30112005184087</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5008145</th>\n",
              "      <td>zyuganov</td>\n",
              "      <td>uc1.31210011533856</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2427113</th>\n",
              "      <td>zyuganov</td>\n",
              "      <td>osu.32435057271629</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9741321</th>\n",
              "      <td>zyuganov</td>\n",
              "      <td>uiug.30112031999763</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4398413</th>\n",
              "      <td>zyuganov</td>\n",
              "      <td>osu.32437010507941</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6085188</th>\n",
              "      <td>a</td>\n",
              "      <td>uc1.31210013217334</td>\n",
              "      <td>12469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6597607</th>\n",
              "      <td>a</td>\n",
              "      <td>uc1.31210021238876</td>\n",
              "      <td>11504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1652341</th>\n",
              "      <td>a</td>\n",
              "      <td>osu.32435030026611</td>\n",
              "      <td>10012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4173500</th>\n",
              "      <td>a</td>\n",
              "      <td>osu.32437000790994</td>\n",
              "      <td>9126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a</td>\n",
              "      <td>ien.35556003701398</td>\n",
              "      <td>2833</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3748276 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             term            volume_id  count\n",
              "8864296        zz  uiug.30112005184087      9\n",
              "5008145  zyuganov   uc1.31210011533856      5\n",
              "2427113  zyuganov   osu.32435057271629      5\n",
              "9741321  zyuganov  uiug.30112031999763      5\n",
              "4398413  zyuganov   osu.32437010507941      5\n",
              "...           ...                  ...    ...\n",
              "6085188         a   uc1.31210013217334  12469\n",
              "6597607         a   uc1.31210021238876  11504\n",
              "1652341         a   osu.32435030026611  10012\n",
              "4173500         a   osu.32437000790994   9126\n",
              "0               a   ien.35556003701398   2833\n",
              "\n",
              "[3748276 rows x 3 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_df.sort_values(by='term', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scikitlearn TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Produce a string of all text in each volume and store in list of strings workset_wc_concat\n",
        "corpus_df['strings'] = (corpus_df['term'] + ' ')*corpus_df['count']\n",
        "\n",
        "workset_wc_concat = []\n",
        "for volume in corpus_df['volume_id'].unique():\n",
        "    workset_wc_concat.append(' '.join(corpus_df[corpus_df['volume_id'] == volume]['strings']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Documentation for vectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "vectorizer = TfidfVectorizer(max_df=0.95,  # max document frequency. Words in more the x% will be ignored\n",
        "                             min_df=.02, # min document frequency. Will ignore words that occur less than x times in the corpus\n",
        "                             sublinear_tf=True, # use logarithmic scale for term frequency.\n",
        "                             )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Denselist` is a list of lists with each list corresponding to a volume. Each sub-list corresponds to the index of the `feature_names`, with each value in the sub-list being the tf-idf score for the corresponding word in the given volume. If a word in `feature_names` does not appear in the volume, it has a score of 0. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vectorize and isolate keywords from documents\n",
        "vectors = vectorizer.fit_transform(workset_wc_concat)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "dense = vectors.todense()\n",
        "denselist = dense.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "653\n",
            "ien.35556003701398\n"
          ]
        }
      ],
      "source": [
        "# Validation\n",
        "print(len(denselist)) # This should be the same as the number of volumes in the workset\n",
        "print(corpus_df['volume_id'].unique()[0]) # This should be a volume_id in the workset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>term</th>\n",
              "      <th>tf-idf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9614</th>\n",
              "      <td>reconversion</td>\n",
              "      <td>0.086422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5809</th>\n",
              "      <td>ianuary</td>\n",
              "      <td>0.072668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6359</th>\n",
              "      <td>iuly</td>\n",
              "      <td>0.072035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6360</th>\n",
              "      <td>iune</td>\n",
              "      <td>0.072018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8359</th>\n",
              "      <td>opa</td>\n",
              "      <td>0.071682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12370</th>\n",
              "      <td>unrra</td>\n",
              "      <td>0.071130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6296</th>\n",
              "      <td>iohn</td>\n",
              "      <td>0.069717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3119</th>\n",
              "      <td>demobilization</td>\n",
              "      <td>0.068821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8664</th>\n",
              "      <td>pauley</td>\n",
              "      <td>0.068446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4357</th>\n",
              "      <td>factfinding</td>\n",
              "      <td>0.067811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8313</th>\n",
              "      <td>oflice</td>\n",
              "      <td>0.065013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7508</th>\n",
              "      <td>mcnutt</td>\n",
              "      <td>0.064283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6726</th>\n",
              "      <td>krug</td>\n",
              "      <td>0.063321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5335</th>\n",
              "      <td>hannegan</td>\n",
              "      <td>0.062588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6362</th>\n",
              "      <td>iustice</td>\n",
              "      <td>0.062573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5821</th>\n",
              "      <td>ickes</td>\n",
              "      <td>0.061689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11216</th>\n",
              "      <td>steelman</td>\n",
              "      <td>0.061315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6358</th>\n",
              "      <td>iudge</td>\n",
              "      <td>0.057952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5807</th>\n",
              "      <td>iames</td>\n",
              "      <td>0.057701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2857</th>\n",
              "      <td>custodian</td>\n",
              "      <td>0.057600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7034</th>\n",
              "      <td>liquidation</td>\n",
              "      <td>0.057593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6299</th>\n",
              "      <td>ioint</td>\n",
              "      <td>0.056853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5808</th>\n",
              "      <td>ian</td>\n",
              "      <td>0.055371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7512</th>\n",
              "      <td>mead</td>\n",
              "      <td>0.054880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7006</th>\n",
              "      <td>lilienthal</td>\n",
              "      <td>0.053775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7177</th>\n",
              "      <td>lune</td>\n",
              "      <td>0.053717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1463</th>\n",
              "      <td>bretton</td>\n",
              "      <td>0.053717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5126</th>\n",
              "      <td>grady</td>\n",
              "      <td>0.053451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1642</th>\n",
              "      <td>byrnes</td>\n",
              "      <td>0.053301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8315</th>\n",
              "      <td>oflicial</td>\n",
              "      <td>0.053074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1385</th>\n",
              "      <td>bottlenecks</td>\n",
              "      <td>0.053074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>810</th>\n",
              "      <td>attlee</td>\n",
              "      <td>0.052801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2730</th>\n",
              "      <td>cpa</td>\n",
              "      <td>0.052594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11737</th>\n",
              "      <td>terminal</td>\n",
              "      <td>0.052446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4273</th>\n",
              "      <td>expediter</td>\n",
              "      <td>0.052390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7492</th>\n",
              "      <td>mckellar</td>\n",
              "      <td>0.051585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6037</th>\n",
              "      <td>infantile</td>\n",
              "      <td>0.051290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12968</th>\n",
              "      <td>wool</td>\n",
              "      <td>0.051176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9813</th>\n",
              "      <td>rents</td>\n",
              "      <td>0.051083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7290</th>\n",
              "      <td>manchuria</td>\n",
              "      <td>0.050889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5810</th>\n",
              "      <td>iapan</td>\n",
              "      <td>0.050805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9939</th>\n",
              "      <td>retailers</td>\n",
              "      <td>0.050418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3321</th>\n",
              "      <td>diflicult</td>\n",
              "      <td>0.049926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11740</th>\n",
              "      <td>terminating</td>\n",
              "      <td>0.049637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4613</th>\n",
              "      <td>flour</td>\n",
              "      <td>0.049546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6297</th>\n",
              "      <td>iohnson</td>\n",
              "      <td>0.049380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13010</th>\n",
              "      <td>wyatt</td>\n",
              "      <td>0.049128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7033</th>\n",
              "      <td>liquidate</td>\n",
              "      <td>0.048778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1271</th>\n",
              "      <td>blast</td>\n",
              "      <td>0.048560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11177</th>\n",
              "      <td>starve</td>\n",
              "      <td>0.048560</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 term    tf-idf\n",
              "9614     reconversion  0.086422\n",
              "5809          ianuary  0.072668\n",
              "6359             iuly  0.072035\n",
              "6360             iune  0.072018\n",
              "8359              opa  0.071682\n",
              "12370           unrra  0.071130\n",
              "6296             iohn  0.069717\n",
              "3119   demobilization  0.068821\n",
              "8664           pauley  0.068446\n",
              "4357      factfinding  0.067811\n",
              "8313           oflice  0.065013\n",
              "7508           mcnutt  0.064283\n",
              "6726             krug  0.063321\n",
              "5335         hannegan  0.062588\n",
              "6362          iustice  0.062573\n",
              "5821            ickes  0.061689\n",
              "11216        steelman  0.061315\n",
              "6358            iudge  0.057952\n",
              "5807            iames  0.057701\n",
              "2857        custodian  0.057600\n",
              "7034      liquidation  0.057593\n",
              "6299            ioint  0.056853\n",
              "5808              ian  0.055371\n",
              "7512             mead  0.054880\n",
              "7006       lilienthal  0.053775\n",
              "7177             lune  0.053717\n",
              "1463          bretton  0.053717\n",
              "5126            grady  0.053451\n",
              "1642           byrnes  0.053301\n",
              "8315         oflicial  0.053074\n",
              "1385      bottlenecks  0.053074\n",
              "810            attlee  0.052801\n",
              "2730              cpa  0.052594\n",
              "11737        terminal  0.052446\n",
              "4273        expediter  0.052390\n",
              "7492         mckellar  0.051585\n",
              "6037        infantile  0.051290\n",
              "12968            wool  0.051176\n",
              "9813            rents  0.051083\n",
              "7290        manchuria  0.050889\n",
              "5810            iapan  0.050805\n",
              "9939        retailers  0.050418\n",
              "3321        diflicult  0.049926\n",
              "11740     terminating  0.049637\n",
              "4613            flour  0.049546\n",
              "6297          iohnson  0.049380\n",
              "13010           wyatt  0.049128\n",
              "7033        liquidate  0.048778\n",
              "1271            blast  0.048560\n",
              "11177          starve  0.048560"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# See tf-idf scores for a given volume\n",
        "pd.DataFrame({\"term\": feature_names, \"tf-idf\":denselist[1]}).sort_values(by='tf-idf', ascending=False).head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dataframe from the denselist\n",
        "\n",
        "dict_from_denselist = {i: item for i, item in enumerate(denselist)}\n",
        "tfidf_df = pd.DataFrame.from_dict(dict_from_denselist)\n",
        "tfidf_df.columns = corpus_df['volume_id'].unique()\n",
        "tfidf_df.index = feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16719, 653)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the tfidf_df to a compressed csv\n",
        "tfidf_df.to_csv('USA Presidential Papers tf-idf.csv.zip', compression='zip')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
